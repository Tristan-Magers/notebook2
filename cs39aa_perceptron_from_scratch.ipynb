{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# CS 39AA - Notebook 2: A Simple Perceptron Model\n",
    "\n",
    "### Objectives of this Notebook\n",
    "* Quick intro/review of logistic regression\n",
    "* Implement/experiment with a toy example of a perceptron\n",
    "* Understand how perceptron is related to logistic regression\n",
    "* See gradient descent implemented with numpy and PyTorch\n",
    "* Implement a basic model w/ PyTorch\n",
    "\n",
    "\n",
    "## I. Logistic Regression\n",
    "\n",
    "### 1: Generate data\n",
    "Import necessary modules and generate synthetic data with appropriate size/dimensions. \n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "N = 100 # total number of observations\n",
    "D_in = 1 # input dimension (i.e. dimension of a single observation's x vector)\n",
    "D_out = 1 # output dimension (i.e. y), so just 1 for this example\n",
    "\n",
    "# Create random input data and derive the 'true' labels/output\n",
    "x = np.random.randn(N, D_in) + 1 \n",
    "def true_y(x_in, n_obs):\n",
    "    def addNoise(x):\n",
    "        if abs(x-1) < 0.1:\n",
    "            return 0.1\n",
    "        elif abs(x-1) < 1.0:\n",
    "            return 0.02\n",
    "        else:\n",
    "            return 0.01\n",
    "\n",
    "    return np.apply_along_axis(lambda x: [int(x < 1) if random.random() < addNoise(x) else int(x > 1)], 1, x_in)\n",
    "    \n",
    "y = true_y(x, N).flatten()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Plot the generated data to see x and y. Note that the y outcomes/labels belong to one of two classes. \n",
    "\n",
    "Positive cases are shown in blue while negative cases are in red. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plt.scatter(x[y == 1,0], y[y == 1], c='blue', alpha=0.4)\n",
    "plt.scatter(x[y == 0,0], y[y == 0], c='red', alpha=0.4)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend(('positive cases', 'negative cases'), loc='upper left')\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2. Fit Logistic Regression Model\n",
    "\n",
    "Let's find estimates for b0 and b1 using a traditional logistic regression model in order to predict/estimate the probability that y is in the positive or negative class for any given x. The function used to isolate the linear function of x is the logit function, defined as: \n",
    "\n",
    "* $\\mathrm{logit}(y_i) = \\beta_0 + \\beta_1*x_i$\n",
    "\n",
    "for $i = 1, ... N$."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg_model = LogisticRegression(random_state=1, max_iter=100, tol=1e-4, solver='sag')\n",
    "logreg_model.fit(x, y)\n",
    "\n",
    "print(f\" beta0 = {logreg_model.intercept_[0]:.4f}\")\n",
    "print(f\" beta1 = {logreg_model.coef_[0][0]:.4f}\")\n",
    "y_pred = logreg_model.predict_proba(x)\n",
    "lr_loss = 1/N * np.square(y - y_pred[:,1]).sum()\n",
    "print(f\" loss (mse) = {lr_loss:.4f}\")\n",
	"\n# I switched the solver to 'sag'",
	"\n# liblinear loss fluxated a lot. averaging 0.07, but going as low as 0.047",
	"\n# I tested all solvers, and 'sag' seemed to give the lowerest average loss",
	"\n# This may be because it is willing to give higher co-ef",
	"\n# 'sag' possibly gives more dramitic results because it is designed for finite set optimization"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### TASK 1\n",
    "\n",
    "TRY COPYING THE ABOVE CELL AND USING A DIFFERENT 'SOLVER'. SEE THE [SCIKIT LEARN DOCUMENTATION ON LOG REG](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.logisticregression.html) IF NECESSARY TO SEE WHAT THE NAMES OF THE OTHER SOLVERS ARE.\n",
    "\n",
    "DESCRIBE THE RESULTS YOU SAW ABOVE. HOW DID THE LOSS AND PARAMETER ESTIMATES CHANGE WHEN YOU USED A DIFFERENT SOLVER? WHICH SOLVER DO YOU THINK IS THE BEST AND WHY?\n",
    "\n",
    "\n",
    "_THIS CONTENTS OF THIS CELL SHOULD BE REMOVED/EDITED AND REPLACED WITH YOUR OWN WORDS AS A REPLY TO THE QUESTION/TASK GIVEN HERE._"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "To find those estimates scikit learn's LogisticRegression model solved an optimization problem. The optimization was to minimize the loss function, defined as:\n",
    "\n",
    "$\\mathrm{Loss} = \\frac{1}{N} \\sum_i^N (y_i - \\hat{y}_i)^2$\n",
    "\n",
    "For $\\hat{y}_i$ we will use the inverse of the logit (i.e. the logistic function, which itself is a generalization of the sigmoid function). \n",
    "\n",
    "$\\hat{y}_i = (1 + e^{-\\beta_1*x_i - \\beta_0})^{-1}$\n",
    "\n",
    "Note that the loss can be written as a function of $\\beta_0$, $\\beta_1$. Thus, estimating these parameters is done by minimizing the loss function with respect to $\\beta_0$, $\\beta_1$.\n",
    "\n",
    "Let's now plot the surface of the loss function with $beta_0$ and $beta_1$ on the x and y axes, respectively. \n",
    "\n",
    "Notice that there are many values of $beta_0$, $\\beta_1$ that may yield a reasonable minimum value. In other words, there is no single point on the surface displayed here where the loss has an obvivous global minimum. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "b1s = np.arange(6, -4.1, -0.5)\n",
    "b0s = np.arange(-6, 4.1, 0.5)\n",
    "surf = np.array( [[1/N * np.square(y - 1 / (1 + np.exp(-1 * (b1s[i]*x[:,0] + b0s[j])))).sum() for j in range(len(b0s))] for i in range(len(b1s))] )\n",
    "df = pd.DataFrame(surf, columns=b0s, index=b1s)\n",
    "p1 = sns.heatmap(df, cbar_kws={'label': 'loss'}, cmap=\"RdYlGn_r\")\n",
    "plt.xlabel(\"beta0\")\n",
    "plt.ylabel(\"beta1\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3: Predict y for a Newly Observed x\n",
    "\n",
    "Try plotting the logistic function for a few values of $\\beta_0$, $\\beta_1$ on top of the data to see if it is a reasonable fit. \n",
    "\n",
    "Note a final step we have not yet described is to determine the threshold of the logistic function for which we will predict a positive or negative case. \n",
    "\n",
    "For example, for $\\beta_0 = \\beta_1 = 3$, if we are given a new observation with $x = 1.5$, and we need to predict whether it is a positive or negative case, then we start by evaluating the logistic function at $x = 1.5$. This yields $\\hat{y} = 0.8176$ (see below). \n",
    "\n",
    "Oftentimes a threshold of 0.5 is employed. Since 0.8176 > 0.5, we would predict this to be a positive case for the typical threshold value. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "b0 = -2.3\n",
    "b1 = 2.8\n",
    "\n",
    "plt.scatter(x[y == 1,0], y[y == 1], c='blue', alpha=0.4)\n",
    "plt.scatter(x[y == 0,0], y[y == 0], c='red', alpha=0.4)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "\n",
    "x_new = 1.5\n",
    "y_hat_new = 1 / (1 + np.exp(-b0 - b1*x_new))\n",
    "plt.scatter(x_new, y_hat_new, color=\"green\")\n",
    "plt.legend(('positive cases', 'negative cases', 'new prediction'), loc='upper left')\n",
    "\n",
    "xes = np.arange(-4, 4, 0.2)\n",
    "plt.plot(xes, 1/(1 + np.exp(-b0 - b1*xes)), 'k--')\n",
    "\n",
    "plt.show()\n",
    "y_hat_new\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## II. A Simple Perceptron\n",
    "\n",
    "\n",
    "### 1. Simple Perceptron w/ NumPy\n",
    "\n",
    "Using the same $x$ and $y$ data as before we will now see how a simple perceptron network with a sigmoid activiation function is equivalent to the logistic regression model above. Rather than $\\beta_0$, $\\beta_1$, the parameters to be estimated are generally referred to as the weight and bias terms ($w$ and $b$, respectively). For this simple case they differ only in name though, such that:\n",
    "* $\\beta_0 = b$\n",
    "* $\\beta_1 = w$\n",
    "\n",
    "To begin we will manually train the perceptron using numpy and gradient descent to minimize the loss. This will involve taking the derivatives of $w$ and $b$ (using the chain rule) to calculate the gradients in an interative process. \n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Randomly initialize parameters to be estimated\n",
    "np.random.seed(42)\n",
    "w = np.random.randn(1)\n",
    "b = np.random.randn(1)\n",
    "\n",
    "# learning rate parameter\n",
    "learning_rate = 5e-1\n",
    "\n",
    "# keep track of loss to see how the optimization performs\n",
    "loss = []\n",
    "\n",
    "# Begin gradient descent using all of the observations in each iteration\n",
    "for i in range(250):\n",
    "\n",
    "    # Forward pass: compute predicted y\n",
    "    lin_pred = w[0] * x[:,0] + b[0]\n",
    "    y_pred = 1 / (1 + np.exp(-1 * lin_pred)) \n",
    "\n",
    "    # Compute and store loss, and print occassionally\n",
    "    loss.append(1/N * np.square(y - y_pred).sum())\n",
    "    if i % 50 == 0:\n",
    "        print(f\"iteration {i}: loss = {loss[i]:.4f}, w = {w[0]:.4f}, b = {b[0]:.4f}\")\n",
    "\n",
    "    # Backprop to compute gradients of w and b with respect to log loss\n",
    "    dloss_dypred = -2.0 / N * (y - y_pred)\n",
    "    dypred_dlinpred = np.exp(-lin_pred) * (1 / np.square(1 + np.exp(-1 * lin_pred)))\n",
    "    dlinpred_dw = x[:,0]\n",
    "    dlinpred_db = 1\n",
    "\n",
    "    # Backprop to compute gradients of w and b with respect to loss and being careful not to sum up intermediate parts\n",
    "    #grad_w = -2 / N * (y - 1 / (1 + np.exp(-1 * lin_pred))) * (1 / np.square(1 + np.exp(-1 * lin_pred))) * x[:,0] * np.exp(-lin_pred) \n",
    "    #grad_b = -2 / N * (y - 1 / (1 + np.exp(-1 * lin_pred))) * (1 / np.square(1 + np.exp(-1 * lin_pred))) * np.exp(-lin_pred) \n",
    "\n",
    "    # Calculate gradients and update weight and bias parameters \n",
    "    grad_w = (dloss_dypred * dypred_dlinpred * dlinpred_dw).sum()\n",
    "    grad_b = (dloss_dypred * dypred_dlinpred * dlinpred_db).sum()\n",
    "    w -= learning_rate * grad_w\n",
    "    b -= learning_rate * grad_b\n",
    "\n",
    "print(f\" w = {w.item():.4f}\")\n",
    "print(f\" b = {b.item():.4f}\")\n",
	"\n\n# DLOSS_DYPRED, DYPRED_DLINPRED, AND DLINPRED_DW all have a dimension of 1 and no clear shape",
	"\n# GRAD_W AND GRAD_B all have a dimension of 0",
	"\n# W AND B all have a dimension of 1 and no shape"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### TASK 2\n",
    "\n",
    "WHAT ARE THE SHAPE AND DIMENSIONS OF DLOSS_DYPRED, DYPRED_DLINPRED, AND DLINPRED_DW?\n",
    "\n",
    "WHAT ARE THE SHAPES AND DIMENSIONS OF GRAD_W? AND GRAD_B?\n",
    "\n",
    "FINALLY, WHAT IS THE SHAPE AND DIMENSIONS OF W AND B? \n",
    "\n",
    "_THIS CONTENTS OF THIS CELL SHOULD BE REMOVED/EDITED AND REPLACED WITH YOUR OWN WORDS AS A REPLY TO THE QUESTION/TASK GIVEN HERE._"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plt.plot(range(0,len(loss)), loss)\n",
    "plt.title(\"training a perceptron w/ numpy\")\n",
    "plt.xlabel(\"iteration of gradient descent\")\n",
    "plt.ylabel(\"loss function value\")\n",
    "plt.show()\n",
	"\n# Using a seed of '99', the curve started twice as high and resembled more of an S-curve",
	"\n# The loss increases, but not significately after 200 iterations, bespite starting over twice as high (3.922)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### TASK 3\n",
    "\n",
    "TWO CODE CELLS ABOVE, THE FIRST LINE OF CODE SETS THE NUMPY RANDOM SEED TO 42. TRY CHANGING THE SEED FROM 42 TO SOME OTHER POSITIVE INTEGER AND THEN RE-RUN THAT CELL AND THE ONE ABOVE THAT PLOTS THE LOSS FUNCTION AT EACH ITERATION (I.E. EPOCH). \n",
    "\n",
    "DOES THE CURVE CHANGE MUCH WHEN A DIFFERENT SEED IS USED? DOES THE FINAL VALUE OF THE LOSS FUNCTION AT THE END OF TRAINING CHANGE? DESCRIBE WHAT YOU SEE. \n",
    "\n",
    "_THIS CONTENTS OF THIS CELL SHOULD BE REMOVED/EDITED AND REPLACED WITH YOUR OWN WORDS AS A REPLY TO THE QUESTION/TASK GIVEN HERE._"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2. Simple Perceptron w/ a PyTorch\n",
    "\n",
    "Now we will train a basic perceptron using PyTorch to take advantage of its ability to perform automatic differentation. This allows us to avoid calculating derivatives manually. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import torch\n",
    "\n",
    "# Randomly initialize weights and other data\n",
    "torch.manual_seed(42)\n",
    "w = torch.randn(1, requires_grad=True)\n",
    "b = torch.randn(1, requires_grad=True)\n",
    "x_tensor = torch.tensor(x)\n",
    "y_tensor = torch.tensor(y)\n",
    "learning_rate = 5e-1\n",
    "losses = []\n",
    "\n",
    "# Carry out gradient descent \n",
    "for i in range(250):\n",
    "\n",
    "    # Forward pass: compute predicted y\n",
    "    lin_pred = w * x_tensor + b\n",
    "    y_pred = lin_pred.flatten().sigmoid()\n",
    "\n",
    "    # Compute and store loss, and print occassionally \n",
    "    loss = 1/N * (y_tensor - y_pred).pow(2).sum()\n",
    "    losses.append(loss.item())\n",
    "    if i % 50 == 0:\n",
    "        print(f\"iteration {i}: loss = {loss.item():.4f}, w = {w[0]:.4f}, b = {b[0]:.4f}\")\n",
    "\n",
    "    # Backprop using PyTorch's automatic differentiation \n",
    "    loss.backward()\n",
    "\n",
    "    # Update parameters but don't include these calculations as part of underlying computational graph\n",
    "    with torch.no_grad():\n",
    "        w -= learning_rate * w.grad\n",
    "        b -= learning_rate * b.grad\n",
    "\n",
    "    # Reset gradients for next iteration \n",
    "    w.grad.zero_()\n",
    "    b.grad.zero_()\n",
    "\n",
    "print(f\" w = {w.item():.4f}\")\n",
    "print(f\" b = {b.item():.4f}\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### TASK 4\n",
    "\n",
    "CHANGE THE LEARNING RATE IN THE CELL ABOVE FROM 0.5 (5E-1) TO 0.05 (5E-2). THEN RE-RUN THE CELL. \n",
    "\n",
    "WHAT HAPPENS TO THE LOSS OVER THE 250 ITERATIONS? HOW MANY ITERATIONS ARE NEEDED TO ARRIVE AT THE SAME LOSS OF AROUND 0.08 OR 0.07 WHEN THE LEARNING RATE IS 0.05?\n",
    "\n",
    "_THIS CONTENTS OF THIS CELL SHOULD BE REMOVED/EDITED AND REPLACED WITH YOUR OWN WORDS AS A REPLY TO THE QUESTION/TASK GIVEN HERE._"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plt.plot(range(0,len(losses)), losses)\n",
    "plt.title(\"training a perceptron w/ pytorch\")\n",
    "plt.xlabel(\"iteration of gradient descent\")\n",
    "plt.ylabel(\"loss function value\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## III. Using PyTorch to Define a Perceptron\n",
    "\n",
    "### 1: A Simple Perceptron using PyTorch\n",
    "\n",
    "Now we will train a basic perceptron using PyTorch to define the model architecture itself, while continuing to take advantage of its automatic differentation. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Randomly initialize weights and other data\n",
    "torch.manual_seed(42)\n",
    "w = torch.randn(1, requires_grad=True).reshape(1,1)\n",
    "b = torch.randn(1, requires_grad=True).reshape(1,1)\n",
    "x_tensor = torch.tensor(x).float()\n",
    "y_tensor = torch.tensor(y).float().reshape(N, D_out)\n",
    "\n",
    "# Define and declare a pytorch perceptron using sigmoid activation function \n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(1, 1),\n",
    "    torch.nn.Sigmoid()\n",
    ")\n",
    "\n",
    "# Define loss function to be used \n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "learning_rate = 5e-1\n",
    "losses = []\n",
    "\n",
    "# Carry out gradient descent \n",
    "for i in range(250):\n",
    "\n",
    "    # Forward pass: compute predicted y\n",
    "    y_pred = model.forward(x_tensor)\n",
    "    \n",
    "    # Compute and store loss, and print occassionally \n",
    "    loss = loss_fn(y_pred, y_tensor)\n",
    "    losses.append(loss.item())\n",
    "    if i % 50 == 0:\n",
    "        print(f\"iteration {i}: loss = {loss.item():.4f}\")\n",
    "\n",
    "    model.zero_grad()\n",
    "\n",
    "    # Backprop using PyTorch's automatic differentiation \n",
    "    loss.backward()\n",
    "\n",
    "    # Update parameters but don't include these calculations as part of underlying computational graph\n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            param.data -= learning_rate * param.grad\n",
    "\n",
    "print(\"w and b:\")\n",
    "for param in model.parameters():\n",
    "    print(f\"    {param.item():.4f}\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### TASK 5\n",
    "\n",
    "TRY CHANGING THE ACTIVATION FUNCTION THAT IS USED ABOVE TO Tanh. CURRENTLY THE Sigmoid ACTIVIATION FUNCTION IS USED, BUT THERE ARE MANY OTHERS (IF YOU'RE CURIOUS, [SEE PYTORCH DOCUMENTATION ON ACTIVATION FUNCTIONS](https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity)). \n",
    "\n",
    "AFTER SWITCHING TO TANH, RE-RUN THE CELL ABOVE AND BELOW. DOES THE VALUE OF THE LOSS AFTER 250 ITERATIONS/EPOCHS CHANGE MUCH? WERE 250 ITERATIONS/EPOCHS NEEDED? OR WOULD MORE OR LESS HAVE BEEN OKAY?\n",
    "\n",
    "DO THE VALUES OF W AND B CHANGE MUCH? WHY DO YOU THINK THIS MIGHT BE? \n",
    "\n",
    "_THIS CONTENTS OF THIS CELL SHOULD BE REMOVED/EDITED AND REPLACED WITH YOUR OWN WORDS AS A REPLY TO THE QUESTION/TASK GIVEN HERE._"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plt.plot(range(0,len(losses)), losses)\n",
    "plt.title(\"training a perceptron as a pytorch model\")\n",
    "plt.xlabel(\"iteration of gradient descent\")\n",
    "plt.ylabel(\"loss function value\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2. PyTorch Optimizer\n",
    "\n",
    "Now we will train a basic perceptron using PyTorch to define the model itself and also to carry out the optimization steps using stochastic gradient descent (a variant of gradient descent that attempts to avoid getting stuck in a local minima). "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Randomly initialize weights and other data\n",
    "torch.manual_seed(42)\n",
    "w = torch.randn(1, requires_grad=True).reshape(1,1)\n",
    "b = torch.randn(1, requires_grad=True).reshape(1,1)\n",
    "x_tensor = torch.tensor(x).float()\n",
    "y_tensor = torch.tensor(y).float().reshape(N, D_out)\n",
    "\n",
    "# Define a Perceptron class \n",
    "#class Perceptron(torch.nn.Module):\n",
    "#    def __init__(self, input_dim):\n",
    "#        super(Perceptron, self).__init__()\n",
    "#        self.lay1 = torch.nn.Linear(input_dim, 1)\n",
    "#        self.act = torch.nn.Sigmoid()\n",
    "#    def forward(self, x):\n",
    "#        output = self.lay1(x)\n",
    "#        output = self.act(output)\n",
    "#        return output\n",
    "\n",
    "# Declare a perceptron instance\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(1, 1),\n",
    "    torch.nn.Sigmoid()\n",
    ")\n",
    "\n",
    "# Define loss function to be used \n",
    "loss_fn = torch.nn.MSELoss()\n",
    "learning_rate = 5e-2\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "losses = []\n",
    "\n",
    "# Carry out gradient descent \n",
    "for i in range(200):\n",
    "\n",
    "    # Forward pass: compute predicted y\n",
    "    y_pred = model.forward(x_tensor)\n",
    "    \n",
    "    # Compute and store loss, and print occassionally \n",
    "    loss = loss_fn(y_pred, y_tensor)\n",
    "    losses.append(loss.item())\n",
    "    if i % 50 == 0:\n",
    "        print(f\"iteration {i}: loss = {loss.item():.4f}\") #\" w = {w[0]:.4f}, b = {b[0]:.4f}\")\n",
    "\n",
    "    # Zero all gradients before backward pass\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Backprop then call optimizer step to update all (relevant) model parameters\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(\"w and b estimates:\")\n",
    "for param in model.parameters():\n",
    "    print(f\"    {param.item():.4f}\")\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### TASK 6\n",
    "\n",
    "TRY CHANGING THE LEARNING RATE ABOVE FROM 5E-2 TO 5E-1, AND THEN RE-RUN THE CELL ABOVE AND THE CELL BELOW. HOW MANY EPOCHS ARE NEEDED FOR THE LOSS TO GO BELOW ~0.05? \n",
    "\n",
    "AFTER THAT, STILL USING THE LEARNING RATE OF 5E-1, TRY CHANGING THE OPTIMIZER FROM torch.nn.Adam to torch.nn.SGD. WHAT HAPPENS TO THE LOSS NOW? HOW MANY EPOCHS ARE NEEDED BEFORE THE LOSS GOES BELOW 0.05? (SEE [PYTORCH DOCUMENNTATION ON OPTIMIZERS](https://pytorch.org/docs/stable/optim.html#algorithms) IF YOU ARE CURIOUS AND WANT TO TRY OTHERS). \n",
    "\n",
    "_THIS CONTENTS OF THIS CELL SHOULD BE REMOVED/EDITED AND REPLACED WITH YOUR OWN WORDS AS A REPLY TO THE QUESTION/TASK GIVEN HERE._"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plt.plot(range(0,len(losses)), losses)\n",
    "plt.title(\"training a perceptron as a pytorch model w/ optimizer\")\n",
    "plt.xlabel(\"iteration of gradient descent\")\n",
    "plt.ylabel(\"loss function value\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "References:\n",
    "* [https://en.wikipedia.org/wiki/Perceptron]()\n",
    "* [https://en.wikipedia.org/wiki/Logistic_regression]()\n",
    "* [https://en.wikipedia.org/wiki/Gradient_descent]()\n",
    "* [https://en.wikipedia.org/wiki/Chain_rule]()\n"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "82f7237e8744fa450af06f16fd99e4f987c4ded027c30bb82e91e6fd7991cebc"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}